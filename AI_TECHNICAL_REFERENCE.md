# ğŸ”§ AI Technical Reference

## ğŸ—ï¸ API Architecture

### æ¦‚è¿°

Huaan Command çš„ AI åŠŸèƒ½é‡‡ç”¨ç»Ÿä¸€çš„ API è®¾è®¡ï¼ŒåŸºäº **OpenAI SDK**ï¼Œæ”¯æŒå¤šä¸ª AI æœåŠ¡å•†ã€‚

### âœ¨ æ ¸å¿ƒç‰¹æ€§

- **ç»Ÿä¸€æ¥å£** - å•ä¸€ `callAI()` æ–¹æ³•å¤„ç†æ‰€æœ‰ AI è°ƒç”¨
- **OpenAI SDK** - ä½¿ç”¨å®˜æ–¹ SDKï¼Œç¨³å®šå¯é 
- **å¤šæœåŠ¡å•†** - æ”¯æŒ OpenAIã€DeepSeekã€Kimi ç­‰
- **æµå¼è¾“å‡º** - åŸç”Ÿæ”¯æŒ SSE æµå¼å“åº”
- **è‡ªåŠ¨é‡è¿** - é…ç½®å˜æ›´æ—¶è‡ªåŠ¨é‡æ–°åˆå§‹åŒ–
- **ç±»å‹å®‰å…¨** - æ¸…æ™°çš„å‚æ•°å’Œè¿”å›ç±»å‹

### ä¸‰å±‚æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åº”ç”¨å±‚ (Application Layer)          â”‚
â”‚  - generateCommand()                 â”‚
â”‚  - diagnoseError()                   â”‚
â”‚  - chat()                            â”‚
â”‚  - analyzeProject()                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ ¸å¿ƒå±‚ (Core Layer)                 â”‚
â”‚  - callAI()  (ç»Ÿä¸€æ¥å£)              â”‚
â”‚  - æµå¼å¤„ç†                          â”‚
â”‚  - é”™è¯¯å¤„ç†                          â”‚
â”‚  - ç»Ÿè®¡æ”¶é›†                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åŸºç¡€å±‚ (Infrastructure Layer)       â”‚
â”‚  - OpenAI SDK                       â”‚
â”‚  - HTTP å®¢æˆ·ç«¯                       â”‚
â”‚  - é…ç½®ç®¡ç†                          â”‚
â”‚  - æœ¬åœ°å­˜å‚¨                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¡ API æ¥å£è®¾è®¡

### æ ¸å¿ƒå‡½æ•°

```javascript
// ç»Ÿä¸€çš„ AI è°ƒç”¨æ¥å£
async callAI({
  messages: Array,        // å¯¹è¯æ¶ˆæ¯
  stream: boolean,        // æ˜¯å¦æµå¼è¾“å‡º
  tools?: Array,          // å·¥å…·è°ƒç”¨
  temperature?: number    // åˆ›é€ æ€§å‚æ•°
}) => Promise<string>
```

### åº”ç”¨å±‚æ¥å£

#### 1. å‘½ä»¤ç”Ÿæˆ
```javascript
async generateCommand(description) {
  return await callAI({
    messages: [
      {
        role: "system",
        content: "ä½ æ˜¯ä¸€ä¸ª Shell å‘½ä»¤ç”Ÿæˆä¸“å®¶..."
      },
      {
        role: "user",
        content: `ç”Ÿæˆå‘½ä»¤ï¼š${description}`
      }
    ],
    temperature: 0.3
  })
}
```

#### 2. é”™è¯¯è¯Šæ–­
```javascript
async diagnoseError(command, output, error) {
  return await callAI({
    messages: [
      {
        role: "system",
        content: "ä½ æ˜¯ä¸€ä¸ªé”™è¯¯è¯Šæ–­ä¸“å®¶..."
      },
      {
        role: "user",
        content: `å‘½ä»¤ï¼š${command}\nè¾“å‡ºï¼š${output}\né”™è¯¯ï¼š${error}`
      }
    ],
    temperature: 0.2
  })
}
```

#### 3. æ™ºèƒ½èŠå¤©
```javascript
async chat(message, history = []) {
  return await callAI({
    messages: [
      {
        role: "system",
        content: "ä½ æ˜¯ Huaan Command çš„ AI åŠ©æ‰‹..."
      },
      ...history,
      {
        role: "user",
        content: message
      }
    ],
    stream: true,
    temperature: 0.7
  })
}
```

## ğŸ”„ æµå¼è¾“å‡ºå®ç°

### Server-Sent Events

```javascript
async function* streamResponse(response) {
  const reader = response.body.getReader()
  const decoder = new TextDecoder()
  
  try {
    while (true) {
      const { done, value } = await reader.read()
      if (done) break
      
      const chunk = decoder.decode(value)
      const lines = chunk.split('\n')
      
      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const data = line.slice(6)
          if (data === '[DONE]') return
          
          try {
            const parsed = JSON.parse(data)
            yield parsed.choices[0]?.delta?.content || ''
          } catch (e) {
            // å¿½ç•¥è§£æé”™è¯¯
          }
        }
      }
    }
  } finally {
    reader.releaseLock()
  }
}
```

### å‰ç«¯å¤„ç†

```javascript
// Vue 3 Composition API
const { streaming, message } = useStreamingResponse()

async function handleStream(prompt) {
  streaming.value = true
  message.value = ''
  
  try {
    for await (const chunk of streamResponse(response)) {
      message.value += chunk
    }
  } finally {
    streaming.value = false
  }
}
```

## ğŸ› ï¸ æœåŠ¡å•†é€‚é…

### OpenAI é€‚é…å™¨

```javascript
class OpenAIAdapter {
  constructor(config) {
    this.client = new OpenAI({
      apiKey: config.apiKey,
      baseURL: config.endpoint
    })
  }
  
  async chat(params) {
    return await this.client.chat.completions.create({
      model: params.model || 'gpt-4o-mini',
      messages: params.messages,
      stream: params.stream,
      temperature: params.temperature
    })
  }
}
```

### DeepSeek é€‚é…å™¨

```javascript
class DeepSeekAdapter {
  constructor(config) {
    this.client = new OpenAI({
      apiKey: config.apiKey,
      baseURL: 'https://api.deepseek.com/v1'
    })
  }
  
  async chat(params) {
    return await this.client.chat.completions.create({
      model: params.model || 'deepseek-chat',
      messages: params.messages,
      stream: params.stream,
      temperature: params.temperature
    })
  }
}
```

### Ollama é€‚é…å™¨

```javascript
class OllamaAdapter {
  constructor(config) {
    this.endpoint = config.endpoint || 'http://localhost:11434/v1'
    this.apiKey = config.apiKey || 'ollama'
  }
  
  async chat(params) {
    const response = await fetch(`${this.endpoint}/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.apiKey}`
      },
      body: JSON.stringify({
        model: params.model || 'llama3.1:8b',
        messages: params.messages,
        stream: params.stream,
        temperature: params.temperature
      })
    })
    
    return response
  }
}
```

## ğŸ“Š é…ç½®ç®¡ç†

### é…ç½®ç»“æ„

```javascript
const aiConfig = {
  // åŸºç¡€é…ç½®
  enabled: true,
  provider: 'openai',
  
  // OpenAI é…ç½®
  openai: {
    apiKey: '',
    model: 'gpt-4o-mini',
    endpoint: 'https://api.openai.com/v1'
  },
  
  // DeepSeek é…ç½®
  deepseek: {
    apiKey: '',
    model: 'deepseek-chat',
    endpoint: 'https://api.deepseek.com/v1'
  },
  
  // Ollama é…ç½®
  ollama: {
    apiKey: 'ollama',
    model: 'llama3.1:8b',
    endpoint: 'http://localhost:11434/v1'
  },
  
  // é«˜çº§é…ç½®
  maxTokens: 4000,
  temperature: 0.7,
  timeout: 30000,
  
  // ç»Ÿè®¡é…ç½®
  enableStats: true,
  statsRetention: 30 // days
}
```

### åŠ¨æ€é…ç½®åˆ‡æ¢

```javascript
class AIManager {
  constructor() {
    this.adapters = new Map()
    this.currentProvider = 'openai'
  }
  
  async switchProvider(provider) {
    if (!this.adapters.has(provider)) {
      await this.initializeAdapter(provider)
    }
    this.currentProvider = provider
  }
  
  async initializeAdapter(provider) {
    const config = aiConfig[provider]
    switch (provider) {
      case 'openai':
        this.adapters.set(provider, new OpenAIAdapter(config))
        break
      case 'deepseek':
        this.adapters.set(provider, new DeepSeekAdapter(config))
        break
      case 'ollama':
        this.adapters.set(provider, new OllamaAdapter(config))
        break
    }
  }
}
```

## âœ… Implementation Checklist

### æ ¸å¿ƒåŠŸèƒ½æ£€æŸ¥

- [ ] **ç»Ÿä¸€ API æ¥å£**
  - [ ] `callAI()` æ–¹æ³•å®ç°
  - [ ] å‚æ•°éªŒè¯
  - [ ] é”™è¯¯å¤„ç†
  
- [ ] **æµå¼è¾“å‡º**
  - [ ] SSE è§£æ
  - [ ] å‰ç«¯æ¸²æŸ“
  - [ ] é”™è¯¯æ¢å¤
  
- [ ] **å¤šæœåŠ¡å•†æ”¯æŒ**
  - [ ] OpenAI é€‚é…å™¨
  - [ ] DeepSeek é€‚é…å™¨
  - [ ] Ollama é€‚é…å™¨
  - [ ] åŠ¨æ€åˆ‡æ¢
  
- [ ] **é…ç½®ç®¡ç†**
  - [ ] é…ç½®éªŒè¯
  - [ ] çƒ­é‡è½½
  - [ ] æœ¬åœ°å­˜å‚¨
  
- [ ] **æ€§èƒ½ä¼˜åŒ–**
  - [ ] è¯·æ±‚å»é‡
  - [ ] ç¼“å­˜æœºåˆ¶
  - [ ] æ‰¹é‡å¤„ç†
  
- [ ] **ç›‘æ§ç»Ÿè®¡**
  - [ ] ä½¿ç”¨é‡ç»Ÿè®¡
  - [ ] é”™è¯¯ç‡ç›‘æ§
  - [ ] æ€§èƒ½æŒ‡æ ‡

### å®‰å…¨æ£€æŸ¥

- [ ] **API Key å®‰å…¨**
  - [ ] åŠ å¯†å­˜å‚¨
  - [ ] ä¼ è¾“åŠ å¯†
  - [ ] æƒé™æ§åˆ¶
  
- [ ] **è¾“å…¥éªŒè¯**
  - [ ] é•¿åº¦é™åˆ¶
  - [ ] å†…å®¹è¿‡æ»¤
  - [ ] æ³¨å…¥é˜²æŠ¤
  
- [ ] **é”™è¯¯å¤„ç†**
  - [ ] æ•æ„Ÿä¿¡æ¯è¿‡æ»¤
  - [ ] é”™è¯¯æ—¥å¿—è®°å½•
  - [ ] ç”¨æˆ·å‹å¥½æç¤º

### æµ‹è¯•æ£€æŸ¥

- [ ] **å•å…ƒæµ‹è¯•**
  - [ ] API æ¥å£æµ‹è¯•
  - [ ] é€‚é…å™¨æµ‹è¯•
  - [ ] é…ç½®ç®¡ç†æµ‹è¯•
  
- [ ] **é›†æˆæµ‹è¯•**
  - [ ] ç«¯åˆ°ç«¯æµç¨‹æµ‹è¯•
  - [ ] å¤šæœåŠ¡å•†åˆ‡æ¢æµ‹è¯•
  - [ ] é”™è¯¯åœºæ™¯æµ‹è¯•
  
- [ ] **æ€§èƒ½æµ‹è¯•**
  - [ ] å¹¶å‘è¯·æ±‚æµ‹è¯•
  - [ ] å†…å­˜ä½¿ç”¨æµ‹è¯•
  - [ ] å“åº”æ—¶é—´æµ‹è¯•

## ğŸ”„ Dynamic Model Loading

### æ¨¡å‹é…ç½®

```javascript
const models = {
  openai: {
    'gpt-4o': {
      name: 'GPT-4o',
      maxTokens: 128000,
      cost: { input: 0.005, output: 0.015 }
    },
    'gpt-4o-mini': {
      name: 'GPT-4o Mini',
      maxTokens: 128000,
      cost: { input: 0.00015, output: 0.0006 }
    }
  },
  deepseek: {
    'deepseek-chat': {
      name: 'DeepSeek Chat',
      maxTokens: 64000,
      cost: { input: 0.0001, output: 0.0002 }
    }
  }
}
```

### åŠ¨æ€åŠ è½½å®ç°

```javascript
class DynamicModelLoader {
  async loadModel(provider, modelName) {
    const modelConfig = models[provider]?.[modelName]
    if (!modelConfig) {
      throw new Error(`Model ${modelName} not found for provider ${provider}`)
    }
    
    // éªŒè¯æ¨¡å‹å¯ç”¨æ€§
    const isAvailable = await this.checkModelAvailability(provider, modelName)
    if (!isAvailable) {
      throw new Error(`Model ${modelName} is not available`)
    }
    
    return modelConfig
  }
  
  async checkModelAvailability(provider, modelName) {
    try {
      const adapter = this.getAdapter(provider)
      const response = await adapter.models.list()
      return response.data.some(model => model.id === modelName)
    } catch (error) {
      return false
    }
  }
}
```

### æ¨¡å‹åˆ‡æ¢

```javascript
async function switchModel(newModel) {
  try {
    // åŠ è½½æ–°æ¨¡å‹é…ç½®
    const modelConfig = await modelLoader.loadModel(
      aiConfig.provider, 
      newModel
    )
    
    // æ›´æ–°é…ç½®
    aiConfig[aiConfig.provider].model = newModel
    
    // é‡æ–°åˆå§‹åŒ–é€‚é…å™¨
    await aiManager.reinitializeAdapter()
    
    // é€šçŸ¥ UI æ›´æ–°
    emit('model-changed', modelConfig)
    
  } catch (error) {
    console.error('Failed to switch model:', error)
    // å›æ»šåˆ°ä¸Šä¸€ä¸ªå¯ç”¨æ¨¡å‹
    await rollbackToPreviousModel()
  }
}
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### è¯·æ±‚ä¼˜åŒ–

```javascript
class RequestOptimizer {
  constructor() {
    this.cache = new Map()
    this.pendingRequests = new Map()
  }
  
  // è¯·æ±‚å»é‡
  async deduplicateRequest(key, requestFn) {
    if (this.pendingRequests.has(key)) {
      return await this.pendingRequests.get(key)
    }
    
    const promise = requestFn()
    this.pendingRequests.set(key, promise)
    
    try {
      const result = await promise
      this.cache.set(key, result)
      return result
    } finally {
      this.pendingRequests.delete(key)
    }
  }
  
  // æ™ºèƒ½ç¼“å­˜
  getCachedResponse(key) {
    const cached = this.cache.get(key)
    if (cached && Date.now() - cached.timestamp < 300000) { // 5åˆ†é’Ÿ
      return cached.data
    }
    return null
  }
}
```

### æ‰¹é‡å¤„ç†

```javascript
class BatchProcessor {
  constructor(batchSize = 5, delay = 100) {
    this.queue = []
    this.batchSize = batchSize
    this.delay = delay
    this.timer = null
  }
  
  add(request) {
    this.queue.push(request)
    this.scheduleBatch()
  }
  
  scheduleBatch() {
    if (this.timer) return
    
    this.timer = setTimeout(() => {
      this.processBatch()
      this.timer = null
    }, this.delay)
  }
  
  async processBatch() {
    const batch = this.queue.splice(0, this.batchSize)
    if (batch.length === 0) return
    
    const promises = batch.map(request => this.executeRequest(request))
    await Promise.allSettled(promises)
    
    // å¦‚æœè¿˜æœ‰å¾…å¤„ç†çš„è¯·æ±‚ï¼Œç»§ç»­å¤„ç†
    if (this.queue.length > 0) {
      this.scheduleBatch()
    }
  }
}
```

## ğŸ” ç›‘æ§ä¸è°ƒè¯•

### æ€§èƒ½ç›‘æ§

```javascript
class PerformanceMonitor {
  constructor() {
    this.metrics = {
      requestCount: 0,
      totalTokens: 0,
      totalCost: 0,
      averageLatency: 0,
      errorRate: 0
    }
  }
  
  recordRequest(startTime, endTime, tokens, cost, error = null) {
    const latency = endTime - startTime
    this.metrics.requestCount++
    this.metrics.totalTokens += tokens
    this.metrics.totalCost += cost
    
    // æ›´æ–°å¹³å‡å»¶è¿Ÿ
    this.metrics.averageLatency = 
      (this.metrics.averageLatency * (this.metrics.requestCount - 1) + latency) / 
      this.metrics.requestCount
    
    // æ›´æ–°é”™è¯¯ç‡
    if (error) {
      this.metrics.errorRate = 
        (this.metrics.errorRate * (this.metrics.requestCount - 1) + 1) / 
        this.metrics.requestCount
    }
  }
  
  getMetrics() {
    return { ...this.metrics }
  }
}
```

### è°ƒè¯•å·¥å…·

```javascript
class AIDebugger {
  static logRequest(provider, model, messages, response) {
    if (process.env.NODE_ENV === 'development') {
      console.group(`ğŸ¤– AI Request - ${provider}/${model}`)
      console.log('Messages:', messages)
      console.log('Response:', response)
      console.groupEnd()
    }
  }
  
  static logError(error, context) {
    if (process.env.NODE_ENV === 'development') {
      console.group('âŒ AI Error')
      console.log('Error:', error)
      console.log('Context:', context)
      console.groupEnd()
    }
  }
}
```

---

## ğŸ“š Additional Resources

- [ğŸ¤– AI Complete User Guide](./AI_COMPLETE_GUIDE.md)
- [ğŸ–¥ï¸ AI Terminal Integration](./AI_TERMINAL_INTEGRATION.md)
- [âš¡ Performance Optimization](./PERFORMANCE_OPTIMIZATION.md)