import { defineStore } from 'pinia'
import { ref, computed } from 'vue'
import OpenAI from 'openai'

// ===========================
// AI æœåŠ¡å•†é…ç½®
// ===========================

export const AI_PROVIDERS = {
  openai: {
    name: 'OpenAI å®˜æ–¹',
    endpoint: 'https://api.openai.com/v1',
    defaultModel: 'gpt-4o-mini',  // é»˜è®¤æ¨¡å‹
    requiresKey: true,
    keyPlaceholder: 'sk-proj-...',
    docs: 'https://platform.openai.com'
  },
  azure: {
    name: 'Azure OpenAI',
    endpoint: 'https://YOUR-RESOURCE.openai.azure.com/openai/deployments/YOUR-DEPLOYMENT',
    defaultModel: 'gpt-4',
    requiresKey: true,
    keyPlaceholder: 'your-azure-api-key',
    docs: 'https://portal.azure.com'
  },
  deepseek: {
    name: 'DeepSeek',
    endpoint: 'https://api.deepseek.com/v1',
    defaultModel: 'deepseek-chat',
    requiresKey: true,
    keyPlaceholder: 'sk-...',
    docs: 'https://platform.deepseek.com'
  },
  zhipu: {
    name: 'æ™ºè°± GLM',
    endpoint: 'https://open.bigmodel.cn/api/paas/v4',
    defaultModel: 'glm-4',
    requiresKey: true,
    keyPlaceholder: 'your-api-key',
    docs: 'https://open.bigmodel.cn'
  },
  moonshot: {
    name: 'Moonshot (Kimi)',
    endpoint: 'https://api.moonshot.cn/v1',
    defaultModel: 'moonshot-v1-8k',
    requiresKey: true,
    keyPlaceholder: 'sk-...',
    docs: 'https://platform.moonshot.cn'
  },
  qwen: {
    name: 'é€šä¹‰åƒé—®',
    endpoint: 'https://dashscope.aliyuncs.com/compatible-mode/v1',
    defaultModel: 'qwen-turbo',
    requiresKey: true,
    keyPlaceholder: 'sk-...',
    docs: 'https://dashscope.console.aliyun.com'
  },
  yi: {
    name: 'é›¶ä¸€ä¸‡ç‰©',
    endpoint: 'https://api.lingyiwanwu.com/v1',
    defaultModel: 'yi-lightning',
    requiresKey: true,
    keyPlaceholder: 'your-api-key',
    docs: 'https://platform.lingyiwanwu.com'
  },
  ollama: {
    name: 'Ollama (æœ¬åœ°)',
    endpoint: 'http://localhost:11434/v1',
    defaultModel: 'llama3.1',
    requiresKey: false,
    keyPlaceholder: 'æœ¬åœ°æœåŠ¡æ— éœ€ API Key',
    docs: 'https://ollama.ai'
  },
  custom: {
    name: 'è‡ªå®šä¹‰æœåŠ¡',
    endpoint: '',
    defaultModel: '',
    requiresKey: true,
    keyPlaceholder: 'æ ¹æ®æœåŠ¡å•†è¦æ±‚å¡«å†™',
    docs: ''
  }
}

// ===========================
// Pinia Store
// ===========================

export const useAIStore = defineStore('ai', () => {
  // ----- é…ç½®çŠ¶æ€ -----
  const provider = ref(localStorage.getItem('ai-provider') || 'openai')
  const apiKey = ref(localStorage.getItem('ai-api-key') || '')
  const apiEndpoint = ref(localStorage.getItem('ai-endpoint') || AI_PROVIDERS.openai.endpoint)
  const model = ref(localStorage.getItem('ai-model') || 'gpt-4o-mini')
  const enabled = ref(localStorage.getItem('ai-enabled') !== 'false')
  
  // ----- è¿è¡Œæ—¶çŠ¶æ€ -----
  const chatMessages = ref([])
  const isGenerating = ref(false)
  
  // ----- ç»Ÿè®¡ä¿¡æ¯ -----
  const stats = ref({
    totalCalls: 0,
    successCalls: 0,
    failedCalls: 0,
    totalTokens: 0
  })

  // ----- OpenAI å®¢æˆ·ç«¯å®ä¾‹ -----
  let client = null

  // ----- Computed -----
  
  // å½“å‰æœåŠ¡å•†é…ç½®
  const currentProvider = computed(() => AI_PROVIDERS[provider.value] || AI_PROVIDERS.openai)
  
  // æ˜¯å¦é…ç½®å®Œæˆ
  const isConfigured = computed(() => {
    if (!enabled.value) return false
    if (currentProvider.value.requiresKey && !apiKey.value) return false
    return !!apiEndpoint.value && !!model.value
  })

  // ----- æ ¸å¿ƒæ–¹æ³• -----

  /**
   * è·å–æˆ–åˆ›å»º OpenAI å®¢æˆ·ç«¯
   */
  function getClient() {
    // å¦‚æœé…ç½®æœªæ”¹å˜ï¼Œå¤ç”¨å®¢æˆ·ç«¯
    if (client) return client
    
    // åˆ›å»ºæ–°å®¢æˆ·ç«¯
    const config = {
      baseURL: apiEndpoint.value,
      dangerouslyAllowBrowser: true  // å…è®¸åœ¨æµè§ˆå™¨ä¸­ä½¿ç”¨
    }
    
    // å¦‚æœéœ€è¦ API Key
    if (currentProvider.value.requiresKey && apiKey.value) {
      config.apiKey = apiKey.value
    }
    
    client = new OpenAI(config)
    return client
  }

  /**
   * é‡ç½®å®¢æˆ·ç«¯ï¼ˆé…ç½®å˜æ›´æ—¶è°ƒç”¨ï¼‰
   */
  function resetClient() {
    client = null
  }

  /**
   * ä¿å­˜é…ç½®
   */
  function saveConfig() {
    localStorage.setItem('ai-provider', provider.value)
    localStorage.setItem('ai-api-key', apiKey.value)
    localStorage.setItem('ai-endpoint', apiEndpoint.value)
    localStorage.setItem('ai-model', model.value)
    localStorage.setItem('ai-enabled', String(enabled.value))
    resetClient()  // é…ç½®å˜æ›´åé‡ç½®å®¢æˆ·ç«¯
  }

  /**
   * åˆ‡æ¢æœåŠ¡å•†
   */
  function switchProvider(newProvider) {
    provider.value = newProvider
    const config = AI_PROVIDERS[newProvider]
    
    // è‡ªåŠ¨æ›´æ–° endpoint å’Œ model
    apiEndpoint.value = config.endpoint
    model.value = config.defaultModel || 'gpt-4o-mini'
    
    saveConfig()
  }

  // ----- AI æ ¸å¿ƒåŠŸèƒ½ -----

  /**
   * è°ƒç”¨ AIï¼ˆç»Ÿä¸€æ¥å£ï¼‰
   * @param {Array} messages - æ¶ˆæ¯æ•°ç»„
   * @param {Object} options - å¯é€‰å‚æ•°
   * @returns {Promise<Object>} AI å“åº”
   */
  async function callAI(messages, options = {}) {
    if (!isConfigured.value) {
      throw new Error('AI æœªé…ç½®ï¼Œè¯·å…ˆåœ¨è®¾ç½®ä¸­é…ç½® API Key å’Œæ¨¡å‹')
    }

    const {
      stream = false,
      onStream = null,
      temperature = 0.7,
      maxTokens = 2000,
      model: customModel = model.value
    } = options

    try {
      isGenerating.value = true
      stats.value.totalCalls++

      const openai = getClient()
      
      const requestParams = {
        model: customModel,
        messages,
        temperature,
        max_tokens: maxTokens,
        stream
      }

      // æµå¼è¾“å‡º
      if (stream && onStream) {
        const streamResponse = await openai.chat.completions.create(requestParams)
        let fullContent = ''
        
        for await (const chunk of streamResponse) {
          const delta = chunk.choices[0]?.delta?.content || ''
          if (delta) {
            fullContent += delta
            onStream(delta, fullContent)
          }
        }
        
        stats.value.successCalls++
        return fullContent
      }
      
      // éæµå¼è¾“å‡º
      const response = await openai.chat.completions.create(requestParams)
      
      stats.value.successCalls++
      if (response.usage) {
        stats.value.totalTokens += response.usage.total_tokens
      }
      
      return response.choices[0].message.content
      
    } catch (error) {
      stats.value.failedCalls++
      console.error('AI è°ƒç”¨å¤±è´¥:', error)
      throw new Error(error.message || 'AI è°ƒç”¨å¤±è´¥')
    } finally {
      isGenerating.value = false
    }
  }

  // ----- åº”ç”¨çº§ AI åŠŸèƒ½ -----

  /**
   * 1. å‘½ä»¤ç”Ÿæˆ / AI å¯¹è¯
   */
  async function generateCommand(description, options = {}) {
    const messages = [
      {
        role: 'system',
        content: `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€å‹å¥½çš„å‘½ä»¤è¡ŒåŠ©æ‰‹ã€‚

**ä½ çš„èƒ½åŠ›ï¼š**
1. å›ç­”å‘½ä»¤ç›¸å…³é—®é¢˜ï¼Œæä¾›æ¸…æ™°çš„è§£é‡Š
2. ç”Ÿæˆå‡†ç¡®çš„ Shell å‘½ä»¤
3. è¯Šæ–­é”™è¯¯å¹¶æä¾›è§£å†³æ–¹æ¡ˆ
4. åˆ†æé¡¹ç›®ç»“æ„å’Œä»£ç 

**å›ç­”è¦æ±‚ï¼š**
- å¦‚æœæ˜¯ç®€å•å‘½ä»¤è¯·æ±‚ï¼Œç›´æ¥è¿”å›å‘½ä»¤ï¼ˆä¸€è¡Œæˆ–å¤šè¡Œç”¨ && è¿æ¥ï¼‰
- å¦‚æœæ˜¯å¤æ‚é—®é¢˜ï¼Œæä¾›æ¸…æ™°çš„ç»“æ„åŒ–å›ç­”ï¼š
  * ä½¿ç”¨ Markdown æ ¼å¼ï¼ˆæ ‡é¢˜ã€åˆ—è¡¨ã€ä»£ç å—ï¼‰
  * ä»£ç ç”¨ \`\`\` åŒ…è£¹
  * å…³é”®æ¦‚å¿µç”¨ \`code\` é«˜äº®
- ä¿æŒç®€æ´ï¼Œé‡ç‚¹çªå‡º
- å¯¹å±é™©æ“ä½œè¦æé†’ç”¨æˆ·`
      },
      {
        role: 'user',
        content: description
      }
    ]

    return await callAI(messages, {
      stream: true,  // é»˜è®¤å¼€å¯æµå¼è¾“å‡º
      temperature: 0.7,
      maxTokens: 2000,
      ...options
    })
  }

  /**
   * 2. é”™è¯¯è¯Šæ–­
   */
  async function diagnoseError(errorMessage, context = '') {
    const messages = [
      {
        role: 'system',
        content: `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é”™è¯¯è¯Šæ–­ä¸“å®¶ã€‚åˆ†æé”™è¯¯ä¿¡æ¯å¹¶æä¾›è§£å†³æ–¹æ¡ˆã€‚
è¿”å›æ ¼å¼ï¼š
1. é”™è¯¯åŸå› ï¼ˆç®€çŸ­ï¼‰
2. è§£å†³æ–¹æ¡ˆï¼ˆå…·ä½“æ­¥éª¤ï¼‰
3. é¢„é˜²å»ºè®®ï¼ˆå¯é€‰ï¼‰`
      },
      {
        role: 'user',
        content: `é”™è¯¯ä¿¡æ¯ï¼š\n${errorMessage}\n\nä¸Šä¸‹æ–‡ï¼š\n${context}`
      }
    ]

    return await callAI(messages, {
      temperature: 0.5,
      maxTokens: 1000
    })
  }

  /**
   * 3. æ—¥å¿—åˆ†æ
   */
  async function analyzeLogs(logs) {
    const messages = [
      {
        role: 'system',
        content: `ä½ æ˜¯ä¸€ä¸ªæ—¥å¿—åˆ†æä¸“å®¶ã€‚åˆ†ææ—¥å¿—å¹¶æå–å…³é”®ä¿¡æ¯ã€‚
é‡ç‚¹å…³æ³¨ï¼š
1. é”™è¯¯å’Œè­¦å‘Š
2. æ€§èƒ½é—®é¢˜
3. å¼‚å¸¸æ¨¡å¼
4. ä¼˜åŒ–å»ºè®®`
      },
      {
        role: 'user',
        content: `è¯·åˆ†æä»¥ä¸‹æ—¥å¿—ï¼š\n\n${logs.substring(0, 4000)}`
      }
    ]

    return await callAI(messages, {
      temperature: 0.4,
      maxTokens: 1500
    })
  }

  /**
   * 4. å·¥ä½œæµæ¨è
   */
  async function recommendWorkflow(projectInfo) {
    const messages = [
      {
        role: 'system',
        content: `ä½ æ˜¯ä¸€ä¸ªé¡¹ç›®å·¥ä½œæµä¸“å®¶ã€‚æ ¹æ®é¡¹ç›®ä¿¡æ¯æ¨èåˆé€‚çš„å·¥ä½œæµã€‚
è¿”å›æ ¼å¼ï¼š
1. æ¨èçš„å·¥ä½œæµåç§°
2. åŒ…å«çš„ä»»åŠ¡åˆ—è¡¨
3. é€‚ç”¨åœºæ™¯è¯´æ˜`
      },
      {
        role: 'user',
        content: `é¡¹ç›®ä¿¡æ¯ï¼š\n${JSON.stringify(projectInfo, null, 2)}`
      }
    ]

    return await callAI(messages, {
      temperature: 0.6,
      maxTokens: 1000
    })
  }

  /**
   * 5. èŠå¤©å¯¹è¯
   */
  async function chat(userMessage, options = {}) {
    // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    const userMsg = {
      id: Date.now(),
      role: 'user',
      content: userMessage,
      timestamp: new Date()
    }
    chatMessages.value.push(userMsg)

    // æ„å»ºæ¶ˆæ¯å†å²ï¼ˆæœ€è¿‘ 10 æ¡ï¼‰
    const messages = chatMessages.value
      .slice(-10)
      .map(msg => ({
        role: msg.role,
        content: msg.content
      }))

    try {
      // AI ä¸´æ—¶æ¶ˆæ¯ï¼ˆç”¨äºæµå¼æ›´æ–°ï¼‰
      const assistantMsg = {
        id: Date.now() + 1,
        role: 'assistant',
        content: '',
        timestamp: new Date()
      }
      chatMessages.value.push(assistantMsg)

      // è·å–æ¶ˆæ¯åœ¨æ•°ç»„ä¸­çš„ç´¢å¼•
      const msgIndex = chatMessages.value.length - 1

      // è°ƒç”¨ AIï¼ˆæ”¯æŒæµå¼ï¼‰
      const content = await callAI(messages, {
        stream: true,
        onStream: (delta, fullContent) => {
          // ç›´æ¥æ›´æ–°æ•°ç»„ä¸­çš„å¯¹è±¡å±æ€§ä»¥è§¦å‘å“åº”å¼
          chatMessages.value[msgIndex].content = fullContent
          if (options.onStream) {
            options.onStream(delta, fullContent)
          }
        },
        temperature: 0.7,
        maxTokens: 2000
      })

      chatMessages.value[msgIndex].content = content
      return chatMessages.value[msgIndex]

    } catch (error) {
      // ç§»é™¤ä¸´æ—¶æ¶ˆæ¯
      chatMessages.value.pop()
      
      // æ·»åŠ é”™è¯¯æ¶ˆæ¯
      const errorMsg = {
        id: Date.now() + 1,
        role: 'assistant',
        content: `âŒ æŠ±æ­‰ï¼ŒAI è°ƒç”¨å¤±è´¥ï¼š${error.message}`,
        timestamp: new Date(),
        isError: true
      }
      chatMessages.value.push(errorMsg)
      throw error
    }
  }

  /**
   * 6. æ™ºèƒ½ä»»åŠ¡æ‰§è¡Œï¼ˆAI Agent æ¨¡å¼ï¼‰
   */
  async function executeIntelligentTask(description, workingDir, executeCommand) {
    // æ£€æµ‹ä»»åŠ¡ç±»å‹
    const isProjectAnalysis = /ç†Ÿæ‚‰|äº†è§£|åˆ†æ|æŸ¥çœ‹|ç†è§£/.test(description) && /é¡¹ç›®|ä»£ç |è¿™ä¸ª/.test(description)
    const isCodeModification = /ä¿®æ”¹|æ·»åŠ |åˆ é™¤|é‡æ„/.test(description)
    
    console.log('ğŸ” ä»»åŠ¡åˆ†æ:', { description, isProjectAnalysis, isCodeModification, hasExecuteCommand: !!executeCommand })
    
    if (isProjectAnalysis && executeCommand) {
      return await analyzeProjectWithCommands(workingDir, executeCommand)
    } else if (isCodeModification) {
      return await modifyCode(description, workingDir, executeCommand)
    } else {
      return await generateCommand(description)
    }
  }

  /**
   * é€šè¿‡ç»ˆç«¯å‘½ä»¤åˆ†æé¡¹ç›®ï¼ˆæ–°æ–¹æ³•ï¼‰
   */
  async function analyzeProjectWithCommands(workingDir, executeCommand) {
    try {
      // ç²¾ç®€å‘½ä»¤åˆ—è¡¨ï¼ˆå¿«é€Ÿå“åº”ï¼‰
      const commands = [
        { cmd: 'ls -la', purpose: 'æŸ¥çœ‹ç›®å½•ç»“æ„' },
        { cmd: 'cat README.md 2>/dev/null || cat readme.md 2>/dev/null || echo "æ— READMEæ–‡ä»¶"', purpose: 'è¯»å–é¡¹ç›®è¯´æ˜' },
        { cmd: 'cat package.json 2>/dev/null || echo "æ— package.json"', purpose: 'è¯»å–é…ç½®æ–‡ä»¶' }
      ]

      // æ‰§è¡Œå‘½ä»¤æ”¶é›†ä¿¡æ¯
      const commandResults = []
      for (const { cmd, purpose } of commands) {
        try {
          const output = await executeCommand(cmd, purpose)
          commandResults.push({
            command: cmd,
            purpose,
            output: output.substring(0, 3000) // é™åˆ¶è¾“å‡ºé•¿åº¦
          })
        } catch (error) {
          commandResults.push({
            command: cmd,
            purpose,
            output: `æ‰§è¡Œå¤±è´¥: ${error.message}`
          })
        }
      }

      // Step 3: AI åˆ†ææ”¶é›†åˆ°çš„ä¿¡æ¯ï¼ˆç²¾ç®€æç¤ºè¯ï¼‰
      const analysisPrompt = `åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œå¿«é€Ÿåˆ†æè¿™ä¸ªé¡¹ç›®ï¼š

${commandResults.map(r => `**${r.purpose}:**
\`\`\`
${r.output}
\`\`\`
`).join('\n')}

è¯·ç®€æ´åœ°å›ç­”ï¼š
1. **é¡¹ç›®ç±»å‹**ï¼šæ˜¯ä»€ä¹ˆç±»å‹çš„é¡¹ç›®ï¼Ÿ
2. **æŠ€æœ¯æ ˆ**ï¼šä½¿ç”¨äº†å“ªäº›ä¸»è¦æŠ€æœ¯ï¼Ÿ
3. **ä¸»è¦åŠŸèƒ½**ï¼šæ ¸å¿ƒåŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ
4. **å¿«é€Ÿä¸Šæ‰‹**ï¼šå¦‚ä½•å¼€å§‹ä½¿ç”¨/å¼€å‘ï¼Ÿ

ç”¨ Markdown æ ¼å¼ï¼Œä¿æŒç®€æ´ã€‚`

      const analysisMessages = [
        {
          role: 'system',
          content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç åˆ†æä¸“å®¶ï¼Œæ“…é•¿å¿«é€Ÿç†è§£é¡¹ç›®ç»“æ„å’ŒæŠ€æœ¯æ ˆã€‚'
        },
        {
          role: 'user',
          content: analysisPrompt
        }
      ]
      
      const analysis = await callAI(analysisMessages, {
        temperature: 0.3,
        maxTokens: 3000
      })
      
      return {
        type: 'project_analysis',
        analysis,
        commandResults
      }
      
    } catch (error) {
      throw new Error(`é¡¹ç›®åˆ†æå¤±è´¥: ${error.message}`)
    }
  }

  /**
   * ä»£ç ä¿®æ”¹
   */
  async function modifyCode(description, workingDir, onProgress) {
    // ç®€æ´æ¨¡å¼ï¼šä¸æ˜¾ç¤ºå†—ä½™çš„è¿›åº¦æç¤º
    // onProgress?.('ğŸ¤– AI æ­£åœ¨ç”Ÿæˆä»£ç ä¿®æ”¹æ–¹æ¡ˆ...')
    
    const messages = [
      {
        role: 'system',
        content: `ä½ æ˜¯ä¸€ä¸ªä»£ç ä¿®æ”¹åŠ©æ‰‹ã€‚ç”¨æˆ·ä¼šæè¿°éœ€è¦çš„ä¿®æ”¹ï¼Œä½ éœ€è¦ï¼š
1. ç¡®å®šéœ€è¦ä¿®æ”¹å“ªäº›æ–‡ä»¶
2. ç”Ÿæˆå…·ä½“çš„ä»£ç ä¿®æ”¹
3. è¿”å› JSON æ ¼å¼ï¼š
{
  "files": [
    {
      "path": "æ–‡ä»¶è·¯å¾„",
      "action": "create|modify|delete",
      "content": "æ–°å†…å®¹ï¼ˆå¦‚æœæ˜¯ create æˆ– modifyï¼‰"
    }
  ],
  "explanation": "ä¿®æ”¹è¯´æ˜"
}`
      },
      {
        role: 'user',
        content: `å·¥ä½œç›®å½•: ${workingDir}\néœ€æ±‚: ${description}`
      }
    ]
    
    const content = await callAI(messages, {
      temperature: 0.2,
      maxTokens: 2000
    })
    
    const jsonMatch = content.match(/\{[\s\S]*\}/)
    if (!jsonMatch) {
      throw new Error('AI è¿”å›æ ¼å¼é”™è¯¯')
    }
    
    const plan = JSON.parse(jsonMatch[0])
    
    return {
      type: 'code_modification',
      plan
    }
  }

  // ----- è¾…åŠ©åŠŸèƒ½ -----

  /**
   * è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼ˆåŠ¨æ€ä» API è·å–ï¼‰
   */
  async function fetchAvailableModels() {
    if (!isConfigured.value) {
      console.warn('AI æœªé…ç½®ï¼Œè¿”å›é»˜è®¤æ¨¡å‹')
      return [{ id: currentProvider.value.defaultModel, name: currentProvider.value.defaultModel }]
    }

    try {
      const openai = getClient()
      
      // è°ƒç”¨ /models ç«¯ç‚¹
      const response = await openai.models.list()
      
      // æå–æ¨¡å‹åˆ—è¡¨
      const models = response.data
        .filter(m => {
          // è¿‡æ»¤å‡ºèŠå¤©æ¨¡å‹ï¼ˆé€šå¸¸åŒ…å« gpt, chat, turbo ç­‰å…³é”®è¯ï¼‰
          const modelId = m.id.toLowerCase()
          return modelId.includes('gpt') || 
                 modelId.includes('chat') || 
                 modelId.includes('turbo') ||
                 modelId.includes('deepseek') ||
                 modelId.includes('glm') ||
                 modelId.includes('moonshot') ||
                 modelId.includes('qwen') ||
                 modelId.includes('yi') ||
                 modelId.includes('llama') ||
                 modelId.includes('mistral')
        })
        .map(m => ({
          id: m.id,
          name: m.id,
          created: m.created,
          owned_by: m.owned_by
        }))
        .sort((a, b) => {
          // æŒ‰åˆ›å»ºæ—¶é—´å€’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
          return (b.created || 0) - (a.created || 0)
        })
      
      if (models.length === 0) {
        console.warn('æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹ï¼Œè¿”å›é»˜è®¤æ¨¡å‹')
        return [{ id: currentProvider.value.defaultModel, name: currentProvider.value.defaultModel }]
      }
      
      return models
      
    } catch (error) {
      console.warn('è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥:', error.message)
      // è¿”å›é»˜è®¤æ¨¡å‹ä½œä¸ºåå¤‡
      return [{ id: currentProvider.value.defaultModel, name: currentProvider.value.defaultModel }]
    }
  }

  /**
   * æ¸…ç©ºèŠå¤©å†å²
   */
  function clearChat() {
    chatMessages.value = []
  }

  /**
   * é‡ç½®ç»Ÿè®¡
   */
  function resetStats() {
    stats.value = {
      totalCalls: 0,
      successCalls: 0,
      failedCalls: 0,
      totalTokens: 0
    }
  }

  // ----- å¯¼å‡º -----

  return {
    // é…ç½®
    provider,
    apiKey,
    apiEndpoint,
    model,
    enabled,
    isConfigured,
    currentProvider,
    saveConfig,
    switchProvider,
    
    // çŠ¶æ€
    isGenerating,
    chatMessages,
    stats,
    
    // æ ¸å¿ƒ AI åŠŸèƒ½
    callAI,
    generateCommand,
    diagnoseError,
    analyzeLogs,
    recommendWorkflow,
    chat,
    executeIntelligentTask,
    
    // è¾…åŠ©åŠŸèƒ½
    fetchAvailableModels,
    clearChat,
    resetStats
  }
})
