import { defineStore } from 'pinia'
import { ref, computed } from 'vue'
import OpenAI from 'openai'

// ===========================
// AI æœåŠ¡å•†é…ç½®
// ===========================

export const AI_PROVIDERS = {
  openai: {
    name: 'OpenAI å®˜æ–¹',
    endpoint: 'https://api.openai.com/v1',
    defaultModel: 'gpt-4o-mini',  // é»˜è®¤æ¨¡å‹
    requiresKey: true,
    keyPlaceholder: 'sk-proj-...',
    docs: 'https://platform.openai.com'
  },
  azure: {
    name: 'Azure OpenAI',
    endpoint: 'https://YOUR-RESOURCE.openai.azure.com/openai/deployments/YOUR-DEPLOYMENT',
    defaultModel: 'gpt-4',
    requiresKey: true,
    keyPlaceholder: 'your-azure-api-key',
    docs: 'https://portal.azure.com'
  },
  deepseek: {
    name: 'DeepSeek',
    endpoint: 'https://api.deepseek.com/v1',
    defaultModel: 'deepseek-chat',
    requiresKey: true,
    keyPlaceholder: 'sk-...',
    docs: 'https://platform.deepseek.com'
  },
  zhipu: {
    name: 'æ™ºè°± GLM',
    endpoint: 'https://open.bigmodel.cn/api/paas/v4',
    defaultModel: 'glm-4',
    requiresKey: true,
    keyPlaceholder: 'your-api-key',
    docs: 'https://open.bigmodel.cn'
  },
  moonshot: {
    name: 'Moonshot (Kimi)',
    endpoint: 'https://api.moonshot.cn/v1',
    defaultModel: 'moonshot-v1-8k',
    requiresKey: true,
    keyPlaceholder: 'sk-...',
    docs: 'https://platform.moonshot.cn'
  },
  qwen: {
    name: 'é€šä¹‰åƒé—®',
    endpoint: 'https://dashscope.aliyuncs.com/compatible-mode/v1',
    defaultModel: 'qwen-turbo',
    requiresKey: true,
    keyPlaceholder: 'sk-...',
    docs: 'https://dashscope.console.aliyun.com'
  },
  yi: {
    name: 'é›¶ä¸€ä¸‡ç‰©',
    endpoint: 'https://api.lingyiwanwu.com/v1',
    defaultModel: 'yi-lightning',
    requiresKey: true,
    keyPlaceholder: 'your-api-key',
    docs: 'https://platform.lingyiwanwu.com'
  },
  ollama: {
    name: 'Ollama (æœ¬åœ°)',
    endpoint: 'http://localhost:11434/v1',
    defaultModel: 'llama3.1',
    requiresKey: false,
    keyPlaceholder: 'æœ¬åœ°æœåŠ¡æ— éœ€ API Key',
    docs: 'https://ollama.ai'
  },
  custom: {
    name: 'è‡ªå®šä¹‰æœåŠ¡',
    endpoint: '',
    defaultModel: '',
    requiresKey: true,
    keyPlaceholder: 'æ ¹æ®æœåŠ¡å•†è¦æ±‚å¡«å†™',
    docs: ''
  }
}

// ===========================
// Pinia Store
// ===========================

export const useAIStore = defineStore('ai', () => {
  // ----- é…ç½®çŠ¶æ€ -----
  const provider = ref(localStorage.getItem('ai-provider') || 'openai')
  const apiKey = ref(localStorage.getItem('ai-api-key') || '')
  const apiEndpoint = ref(localStorage.getItem('ai-endpoint') || AI_PROVIDERS.openai.endpoint)
  const model = ref(localStorage.getItem('ai-model') || 'gpt-4o-mini')
  const enabled = ref(localStorage.getItem('ai-enabled') !== 'false')
  
  // ----- è¿è¡Œæ—¶çŠ¶æ€ -----
  const chatMessages = ref([])
  const isGenerating = ref(false)
  
  // ----- ç»Ÿè®¡ä¿¡æ¯ -----
  const stats = ref({
    totalCalls: 0,
    successCalls: 0,
    failedCalls: 0,
    totalTokens: 0
  })

  // ----- OpenAI å®¢æˆ·ç«¯å®ä¾‹ -----
  let client = null

  // ----- Computed -----
  
  // å½“å‰æœåŠ¡å•†é…ç½®
  const currentProvider = computed(() => AI_PROVIDERS[provider.value] || AI_PROVIDERS.openai)
  
  // æ˜¯å¦é…ç½®å®Œæˆ
  const isConfigured = computed(() => {
    if (!enabled.value) return false
    if (currentProvider.value.requiresKey && !apiKey.value) return false
    return !!apiEndpoint.value && !!model.value
  })

  // ----- æ ¸å¿ƒæ–¹æ³• -----

  /**
   * è·å–æˆ–åˆ›å»º OpenAI å®¢æˆ·ç«¯
   */
  function getClient() {
    // å¦‚æœé…ç½®æœªæ”¹å˜ï¼Œå¤ç”¨å®¢æˆ·ç«¯
    if (client) return client
    
    // åˆ›å»ºæ–°å®¢æˆ·ç«¯
    const config = {
      baseURL: apiEndpoint.value,
      dangerouslyAllowBrowser: true  // å…è®¸åœ¨æµè§ˆå™¨ä¸­ä½¿ç”¨
    }
    
    // å¦‚æœéœ€è¦ API Key
    if (currentProvider.value.requiresKey && apiKey.value) {
      config.apiKey = apiKey.value
    }
    
    client = new OpenAI(config)
    return client
  }

  /**
   * é‡ç½®å®¢æˆ·ç«¯ï¼ˆé…ç½®å˜æ›´æ—¶è°ƒç”¨ï¼‰
   */
  function resetClient() {
    client = null
  }

  /**
   * ä¿å­˜é…ç½®
   */
  function saveConfig() {
    localStorage.setItem('ai-provider', provider.value)
    localStorage.setItem('ai-api-key', apiKey.value)
    localStorage.setItem('ai-endpoint', apiEndpoint.value)
    localStorage.setItem('ai-model', model.value)
    localStorage.setItem('ai-enabled', String(enabled.value))
    resetClient()  // é…ç½®å˜æ›´åé‡ç½®å®¢æˆ·ç«¯
  }

  /**
   * åˆ‡æ¢æœåŠ¡å•†
   */
  function switchProvider(newProvider) {
    provider.value = newProvider
    const config = AI_PROVIDERS[newProvider]
    
    // è‡ªåŠ¨æ›´æ–° endpoint å’Œ model
    apiEndpoint.value = config.endpoint
    model.value = config.defaultModel || 'gpt-4o-mini'
    
    saveConfig()
  }

  // ----- AI æ ¸å¿ƒåŠŸèƒ½ -----

  /**
   * è°ƒç”¨ AIï¼ˆç»Ÿä¸€æ¥å£ï¼‰
   * @param {Array} messages - æ¶ˆæ¯æ•°ç»„
   * @param {Object} options - å¯é€‰å‚æ•°
   * @returns {Promise<Object>} AI å“åº”
   */
  async function callAI(messages, options = {}) {
    if (!isConfigured.value) {
      throw new Error('AI æœªé…ç½®ï¼Œè¯·å…ˆåœ¨è®¾ç½®ä¸­é…ç½® API Key å’Œæ¨¡å‹')
    }

    const {
      stream = false,
      onStream = null,
      temperature = 0.7,
      maxTokens = 2000,
      model: customModel = model.value
    } = options

    try {
      isGenerating.value = true
      stats.value.totalCalls++

      const openai = getClient()

      const requestParams = {
        model: customModel,
        messages,
        temperature,
        max_tokens: maxTokens,
        stream
      }

      console.log('ğŸ“¤ å‘é€ AI è¯·æ±‚:', {
        model: customModel,
        messageCount: messages.length,
        stream,
        temperature,
        max_tokens: maxTokens
      })

      // æµå¼è¾“å‡º
      if (stream && onStream) {
        const streamResponse = await openai.chat.completions.create(requestParams)
        let fullContent = ''
        
        for await (const chunk of streamResponse) {
          const delta = chunk.choices[0]?.delta?.content || ''
          if (delta) {
            fullContent += delta
            onStream(delta, fullContent)
          }
        }
        
        stats.value.successCalls++
        return fullContent
      }
      
      // éæµå¼è¾“å‡º
      const response = await openai.chat.completions.create(requestParams)

      // æ·»åŠ å“åº”æ£€æŸ¥å’Œæ—¥å¿—
      console.log('ğŸ” AI å“åº”:', {
        responseExists: !!response,
        responseType: typeof response,
        hasChoices: !!response?.choices,
        choicesLength: response?.choices?.length,
        firstChoice: response?.choices?.[0],
        fullResponse: response
      })

      // æ£€æŸ¥å“åº”æœ¬èº«æ˜¯å¦å­˜åœ¨
      if (!response) {
        throw new Error('AI è¿”å›ç©ºå“åº”')
      }

      // æ£€æŸ¥å“åº”æ ¼å¼
      if (!response.choices || response.choices.length === 0) {
        throw new Error('AI è¿”å›æ ¼å¼é”™è¯¯ï¼šæ²¡æœ‰ choices')
      }

      if (!response.choices[0].message) {
        throw new Error('AI è¿”å›æ ¼å¼é”™è¯¯ï¼šæ²¡æœ‰ message')
      }

      stats.value.successCalls++
      if (response.usage) {
        stats.value.totalTokens += response.usage.total_tokens
      }

      return response.choices[0].message.content
      
    } catch (error) {
      stats.value.failedCalls++
      console.error('âŒ AI è°ƒç”¨å¤±è´¥:', {
        message: error.message,
        name: error.name,
        stack: error.stack,
        fullError: error
      })

      // æä¾›æ›´å‹å¥½çš„é”™è¯¯æç¤º
      let friendlyMessage = 'AI è°ƒç”¨å¤±è´¥'

      if (error.message?.includes('network') || error.message?.includes('fetch')) {
        friendlyMessage = 'ç½‘ç»œè¿æ¥å¤±è´¥,è¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®æˆ– API ç«¯ç‚¹é…ç½®'
      } else if (error.message?.includes('401') || error.message?.includes('Unauthorized')) {
        friendlyMessage = 'API Key æ— æ•ˆæˆ–å·²è¿‡æœŸ,è¯·æ£€æŸ¥è®¾ç½®ä¸­çš„ API Key'
      } else if (error.message?.includes('429') || error.message?.includes('rate limit')) {
        friendlyMessage = 'API è°ƒç”¨æ¬¡æ•°è¶…é™,è¯·ç¨åé‡è¯•æˆ–å‡çº§å¥—é¤'
      } else if (error.message?.includes('timeout')) {
        friendlyMessage = 'è¯·æ±‚è¶…æ—¶,è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•'
      } else if (error.message) {
        friendlyMessage = error.message
      }

      throw new Error(friendlyMessage)
    } finally {
      isGenerating.value = false
    }
  }

  // ----- åº”ç”¨çº§ AI åŠŸèƒ½ -----

  /**
   * 1. å‘½ä»¤ç”Ÿæˆ / AI å¯¹è¯
   */
  async function generateCommand(description, options = {}) {
    const messages = [
      {
        role: 'system',
        content: `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€å‹å¥½çš„å‘½ä»¤è¡ŒåŠ©æ‰‹ã€‚

**ä½ çš„èƒ½åŠ›ï¼š**
1. å›ç­”å‘½ä»¤ç›¸å…³é—®é¢˜ï¼Œæä¾›æ¸…æ™°çš„è§£é‡Š
2. ç”Ÿæˆå‡†ç¡®çš„ Shell å‘½ä»¤
3. è¯Šæ–­é”™è¯¯å¹¶æä¾›è§£å†³æ–¹æ¡ˆ
4. åˆ†æé¡¹ç›®ç»“æ„å’Œä»£ç 

**å›ç­”è¦æ±‚ï¼š**
- å¦‚æœæ˜¯ç®€å•å‘½ä»¤è¯·æ±‚ï¼Œç›´æ¥è¿”å›å‘½ä»¤ï¼ˆä¸€è¡Œæˆ–å¤šè¡Œç”¨ && è¿æ¥ï¼‰
- å¦‚æœæ˜¯å¤æ‚é—®é¢˜ï¼Œæä¾›æ¸…æ™°çš„ç»“æ„åŒ–å›ç­”ï¼š
  * ä½¿ç”¨ Markdown æ ¼å¼ï¼ˆæ ‡é¢˜ã€åˆ—è¡¨ã€ä»£ç å—ï¼‰
  * ä»£ç ç”¨ \`\`\` åŒ…è£¹
  * å…³é”®æ¦‚å¿µç”¨ \`code\` é«˜äº®
- ä¿æŒç®€æ´ï¼Œé‡ç‚¹çªå‡º
- å¯¹å±é™©æ“ä½œè¦æé†’ç”¨æˆ·`
      },
      {
        role: 'user',
        content: description
      }
    ]

    return await callAI(messages, {
      stream: true,  // é»˜è®¤å¼€å¯æµå¼è¾“å‡º
      temperature: 0.7,
      maxTokens: 2000,
      ...options
    })
  }

  /**
   * 2. é”™è¯¯è¯Šæ–­
   */
  async function diagnoseError(errorMessage, context = '') {
    const messages = [
      {
        role: 'system',
        content: `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é”™è¯¯è¯Šæ–­ä¸“å®¶ã€‚åˆ†æé”™è¯¯ä¿¡æ¯å¹¶æä¾›è§£å†³æ–¹æ¡ˆã€‚
è¿”å›æ ¼å¼ï¼š
1. é”™è¯¯åŸå› ï¼ˆç®€çŸ­ï¼‰
2. è§£å†³æ–¹æ¡ˆï¼ˆå…·ä½“æ­¥éª¤ï¼‰
3. é¢„é˜²å»ºè®®ï¼ˆå¯é€‰ï¼‰`
      },
      {
        role: 'user',
        content: `é”™è¯¯ä¿¡æ¯ï¼š\n${errorMessage}\n\nä¸Šä¸‹æ–‡ï¼š\n${context}`
      }
    ]

    return await callAI(messages, {
      temperature: 0.5,
      maxTokens: 1000
    })
  }

  /**
   * 3. æ—¥å¿—åˆ†æ
   */
  async function analyzeLogs(logs) {
    const messages = [
      {
        role: 'system',
        content: `ä½ æ˜¯ä¸€ä¸ªæ—¥å¿—åˆ†æä¸“å®¶ã€‚åˆ†ææ—¥å¿—å¹¶æå–å…³é”®ä¿¡æ¯ã€‚
é‡ç‚¹å…³æ³¨ï¼š
1. é”™è¯¯å’Œè­¦å‘Š
2. æ€§èƒ½é—®é¢˜
3. å¼‚å¸¸æ¨¡å¼
4. ä¼˜åŒ–å»ºè®®`
      },
      {
        role: 'user',
        content: `è¯·åˆ†æä»¥ä¸‹æ—¥å¿—ï¼š\n\n${logs.substring(0, 4000)}`
      }
    ]

    return await callAI(messages, {
      temperature: 0.4,
      maxTokens: 1500
    })
  }

  /**
   * 4. å·¥ä½œæµæ¨è
   */
  async function recommendWorkflow(projectInfo) {
    const messages = [
      {
        role: 'system',
        content: `ä½ æ˜¯ä¸€ä¸ªé¡¹ç›®å·¥ä½œæµä¸“å®¶ã€‚æ ¹æ®é¡¹ç›®ä¿¡æ¯æ¨èåˆé€‚çš„å·¥ä½œæµã€‚
è¿”å›æ ¼å¼ï¼š
1. æ¨èçš„å·¥ä½œæµåç§°
2. åŒ…å«çš„ä»»åŠ¡åˆ—è¡¨
3. é€‚ç”¨åœºæ™¯è¯´æ˜`
      },
      {
        role: 'user',
        content: `é¡¹ç›®ä¿¡æ¯ï¼š\n${JSON.stringify(projectInfo, null, 2)}`
      }
    ]

    return await callAI(messages, {
      temperature: 0.6,
      maxTokens: 1000
    })
  }

  /**
   * 5. èŠå¤©å¯¹è¯
   */
  async function chat(userMessage, options = {}) {
    // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    const userMsg = {
      id: Date.now(),
      role: 'user',
      content: userMessage,
      timestamp: new Date()
    }
    chatMessages.value.push(userMsg)

    // æ„å»ºæ¶ˆæ¯å†å²ï¼ˆæœ€è¿‘ 10 æ¡ï¼‰
    const messages = chatMessages.value
      .slice(-10)
      .map(msg => ({
        role: msg.role,
        content: msg.content
      }))

    try {
      // AI ä¸´æ—¶æ¶ˆæ¯ï¼ˆç”¨äºæµå¼æ›´æ–°ï¼‰
      const assistantMsg = {
        id: Date.now() + 1,
        role: 'assistant',
        content: '',
        timestamp: new Date()
      }
      chatMessages.value.push(assistantMsg)

      // è·å–æ¶ˆæ¯åœ¨æ•°ç»„ä¸­çš„ç´¢å¼•
      const msgIndex = chatMessages.value.length - 1

      // è°ƒç”¨ AIï¼ˆæ”¯æŒæµå¼ï¼‰
      const content = await callAI(messages, {
        stream: true,
        onStream: (delta, fullContent) => {
          // ç›´æ¥æ›´æ–°æ•°ç»„ä¸­çš„å¯¹è±¡å±æ€§ä»¥è§¦å‘å“åº”å¼
          chatMessages.value[msgIndex].content = fullContent
          if (options.onStream) {
            options.onStream(delta, fullContent)
          }
        },
        temperature: 0.7,
        maxTokens: 2000
      })

      chatMessages.value[msgIndex].content = content
      return chatMessages.value[msgIndex]

    } catch (error) {
      // ç§»é™¤ä¸´æ—¶æ¶ˆæ¯
      chatMessages.value.pop()
      
      // æ·»åŠ é”™è¯¯æ¶ˆæ¯
      const errorMsg = {
        id: Date.now() + 1,
        role: 'assistant',
        content: `âŒ æŠ±æ­‰ï¼ŒAI è°ƒç”¨å¤±è´¥ï¼š${error.message}`,
        timestamp: new Date(),
        isError: true
      }
      chatMessages.value.push(errorMsg)
      throw error
    }
  }

  /**
   * 6. æ™ºèƒ½ä»»åŠ¡æ‰§è¡Œï¼ˆAI Agent æ¨¡å¼ï¼‰
   */
  async function executeIntelligentTask(description, workingDir, executeCommand) {
    // æ£€æµ‹ä»»åŠ¡ç±»å‹
    const isProjectAnalysis = /ç†Ÿæ‚‰|äº†è§£|åˆ†æ|æŸ¥çœ‹|ç†è§£/.test(description) && /é¡¹ç›®|ä»£ç |è¿™ä¸ª/.test(description)
    const isCodeModification = /ä¿®æ”¹|æ·»åŠ |åˆ é™¤|é‡æ„/.test(description)
    
    console.log('ğŸ” ä»»åŠ¡åˆ†æ:', { description, isProjectAnalysis, isCodeModification, hasExecuteCommand: !!executeCommand })
    
    if (isProjectAnalysis && executeCommand) {
      return await analyzeProjectWithCommands(workingDir, executeCommand)
    } else if (isCodeModification) {
      return await modifyCode(description, workingDir, executeCommand)
    } else {
      return await generateCommand(description)
    }
  }

  /**
   * é€šè¿‡ç»ˆç«¯å‘½ä»¤åˆ†æé¡¹ç›®ï¼ˆæ–°æ–¹æ³•ï¼‰
   */
  async function analyzeProjectWithCommands(workingDir, executeCommand) {
    try {
      console.log('ğŸ“‚ å¼€å§‹åˆ†æé¡¹ç›®:', workingDir)

      // æ‰©å±•å‘½ä»¤åˆ—è¡¨ï¼Œè·å–æ›´å¤šé¡¹ç›®ä¿¡æ¯
      const commands = [
        { cmd: 'ls -la', purpose: 'æŸ¥çœ‹ç›®å½•ç»“æ„' },
        { cmd: 'cat README.md 2>/dev/null || cat readme.md 2>/dev/null || cat README 2>/dev/null || echo "æ— READMEæ–‡ä»¶"', purpose: 'è¯»å–é¡¹ç›®è¯´æ˜' },
        { cmd: 'cat package.json 2>/dev/null || echo "æ— package.json"', purpose: 'è¯»å–package.jsoné…ç½®' },
        { cmd: 'cat Cargo.toml 2>/dev/null || echo "æ— Cargo.toml"', purpose: 'è¯»å–Rusté¡¹ç›®é…ç½®' },
        { cmd: 'cat tauri.conf.json 2>/dev/null || cat src-tauri/tauri.conf.json 2>/dev/null || echo "æ— taurié…ç½®"', purpose: 'è¯»å–Taurié…ç½®' },
        { cmd: 'ls -la src/ 2>/dev/null || echo "æ— srcç›®å½•"', purpose: 'æŸ¥çœ‹srcç›®å½•' },
        { cmd: 'ls -la src-tauri/src/ 2>/dev/null || echo "æ— src-tauriç›®å½•"', purpose: 'æŸ¥çœ‹Rustæºç ç›®å½•' },
        { cmd: 'find . -name "*.vue" -o -name "*.js" -o -name "*.ts" | head -20', purpose: 'æŸ¥æ‰¾ä¸»è¦æºæ–‡ä»¶' },
        { cmd: 'cat .gitignore 2>/dev/null | head -20 || echo "æ— .gitignore"', purpose: 'æŸ¥çœ‹gité…ç½®' }
      ]

      // æ‰§è¡Œå‘½ä»¤æ”¶é›†ä¿¡æ¯
      const commandResults = []
      for (const { cmd, purpose } of commands) {
        try {
          console.log(`  âš™ï¸ æ‰§è¡Œå‘½ä»¤: ${cmd}`)
          const output = await executeCommand(cmd, purpose)
          console.log(`  âœ… ${purpose}: ${output.length} å­—ç¬¦`)
          commandResults.push({
            command: cmd,
            purpose,
            output: output.substring(0, 5000) // å¢åŠ è¾“å‡ºé•¿åº¦é™åˆ¶
          })
        } catch (error) {
          console.error(`  âŒ ${purpose} å¤±è´¥:`, error)
          commandResults.push({
            command: cmd,
            purpose,
            output: `æ‰§è¡Œå¤±è´¥: ${error.message}`
          })
        }
      }

      console.log('ğŸ“Š å‘½ä»¤æ‰§è¡Œå®Œæˆï¼Œæ”¶é›†äº†', commandResults.length, 'ä¸ªç»“æœ')

      // æ„å»ºæ›´è¯¦ç»†çš„åˆ†ææç¤ºè¯
      const analysisPrompt = `ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„ä»£ç æ¶æ„å¸ˆã€‚åŸºäºä»¥ä¸‹ç»ˆç«¯å‘½ä»¤çš„è¾“å‡ºï¼Œæ·±å…¥åˆ†æè¿™ä¸ªé¡¹ç›®ï¼š

**å‘½ä»¤è¾“å‡ºï¼š**
${commandResults.map(r => `
### ${r.purpose}
\`\`\`
${r.output}
\`\`\`
`).join('\n')}

**è¯·è¯¦ç»†åˆ†æï¼š**

1. **é¡¹ç›®æ¦‚è¿°**
   - é¡¹ç›®åç§°å’Œç‰ˆæœ¬
   - é¡¹ç›®ç±»å‹ï¼ˆWebåº”ç”¨/æ¡Œé¢åº”ç”¨/CLIå·¥å…·ç­‰ï¼‰
   - ä¸»è¦ç”¨é€”å’Œç›®æ ‡ç”¨æˆ·

2. **æŠ€æœ¯æ¶æ„**
   - å‰ç«¯æ¡†æ¶å’Œç‰ˆæœ¬
   - åç«¯æŠ€æœ¯æ ˆ
   - æ„å»ºå·¥å…·é“¾
   - å…³é”®ä¾èµ–åº“

3. **é¡¹ç›®ç»“æ„**
   - æ ¸å¿ƒç›®å½•è¯´æ˜
   - ä¸»è¦æºæ–‡ä»¶ç±»å‹
   - é…ç½®æ–‡ä»¶ä½œç”¨

4. **å¼€å‘æŒ‡å—**
   - ç¯å¢ƒè¦æ±‚
   - å®‰è£…æ­¥éª¤
   - å¼€å‘å‘½ä»¤
   - æ„å»ºå’Œéƒ¨ç½²æµç¨‹

5. **ç‰¹è‰²åŠŸèƒ½**
   - æ ¸å¿ƒåŠŸèƒ½æ¨¡å—
   - æŠ€æœ¯äº®ç‚¹
   - åˆ›æ–°ä¹‹å¤„

ç”¨æ¸…æ™°çš„ Markdown æ ¼å¼è¾“å‡ºï¼ŒåŒ…å«ä»£ç å—ã€åˆ—è¡¨å’Œå¼ºè°ƒã€‚è¦å…·ä½“ã€æ·±å…¥ï¼Œä¸è¦æ³›æ³›è€Œè°ˆã€‚`

      const analysisMessages = [
        {
          role: 'system',
          content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç åˆ†æä¸“å®¶ï¼Œæ“…é•¿å¿«é€Ÿç†è§£é¡¹ç›®ç»“æ„å’ŒæŠ€æœ¯æ ˆã€‚è¯·æä¾›è¯¦ç»†ã€æ·±å…¥çš„åˆ†æï¼Œè€Œä¸æ˜¯æ³›æ³›è€Œè°ˆã€‚'
        },
        {
          role: 'user',
          content: analysisPrompt
        }
      ]

      const analysis = await callAI(analysisMessages, {
        temperature: 0.3,
        maxTokens: 8000  // å¢åŠ tokené™åˆ¶ï¼Œæ”¯æŒæ›´è¯¦ç»†çš„åˆ†æ
      })
      
      return {
        type: 'project_analysis',
        analysis,
        commandResults
      }
      
    } catch (error) {
      throw new Error(`é¡¹ç›®åˆ†æå¤±è´¥: ${error.message}`)
    }
  }

  /**
   * ä»£ç ä¿®æ”¹
   */
  async function modifyCode(description, workingDir, onProgress) {
    // ç®€æ´æ¨¡å¼ï¼šä¸æ˜¾ç¤ºå†—ä½™çš„è¿›åº¦æç¤º
    // onProgress?.('ğŸ¤– AI æ­£åœ¨ç”Ÿæˆä»£ç ä¿®æ”¹æ–¹æ¡ˆ...')
    
    const messages = [
      {
        role: 'system',
        content: `ä½ æ˜¯ä¸€ä¸ªä»£ç ä¿®æ”¹åŠ©æ‰‹ã€‚ç”¨æˆ·ä¼šæè¿°éœ€è¦çš„ä¿®æ”¹ï¼Œä½ éœ€è¦ï¼š
1. ç¡®å®šéœ€è¦ä¿®æ”¹å“ªäº›æ–‡ä»¶
2. ç”Ÿæˆå…·ä½“çš„ä»£ç ä¿®æ”¹
3. è¿”å› JSON æ ¼å¼ï¼š
{
  "files": [
    {
      "path": "æ–‡ä»¶è·¯å¾„",
      "action": "create|modify|delete",
      "content": "æ–°å†…å®¹ï¼ˆå¦‚æœæ˜¯ create æˆ– modifyï¼‰"
    }
  ],
  "explanation": "ä¿®æ”¹è¯´æ˜"
}`
      },
      {
        role: 'user',
        content: `å·¥ä½œç›®å½•: ${workingDir}\néœ€æ±‚: ${description}`
      }
    ]
    
    const content = await callAI(messages, {
      temperature: 0.2,
      maxTokens: 2000
    })
    
    const jsonMatch = content.match(/\{[\s\S]*\}/)
    if (!jsonMatch) {
      throw new Error('AI è¿”å›æ ¼å¼é”™è¯¯')
    }
    
    const plan = JSON.parse(jsonMatch[0])
    
    return {
      type: 'code_modification',
      plan
    }
  }

  // ----- è¾…åŠ©åŠŸèƒ½ -----

  /**
   * è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼ˆåŠ¨æ€ä» API è·å–ï¼‰
   */
  async function fetchAvailableModels() {
    if (!isConfigured.value) {
      console.warn('AI æœªé…ç½®ï¼Œè¿”å›é»˜è®¤æ¨¡å‹')
      return [{ id: currentProvider.value.defaultModel, name: currentProvider.value.defaultModel }]
    }

    try {
      const openai = getClient()
      
      // è°ƒç”¨ /models ç«¯ç‚¹
      const response = await openai.models.list()
      
      // æå–æ¨¡å‹åˆ—è¡¨
      const models = response.data
        .filter(m => {
          // è¿‡æ»¤å‡ºèŠå¤©æ¨¡å‹ï¼ˆé€šå¸¸åŒ…å« gpt, chat, turbo ç­‰å…³é”®è¯ï¼‰
          const modelId = m.id.toLowerCase()
          return modelId.includes('gpt') || 
                 modelId.includes('chat') || 
                 modelId.includes('turbo') ||
                 modelId.includes('deepseek') ||
                 modelId.includes('glm') ||
                 modelId.includes('moonshot') ||
                 modelId.includes('qwen') ||
                 modelId.includes('yi') ||
                 modelId.includes('llama') ||
                 modelId.includes('mistral')
        })
        .map(m => ({
          id: m.id,
          name: m.id,
          created: m.created,
          owned_by: m.owned_by
        }))
        .sort((a, b) => {
          // æŒ‰åˆ›å»ºæ—¶é—´å€’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
          return (b.created || 0) - (a.created || 0)
        })
      
      if (models.length === 0) {
        console.warn('æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹ï¼Œè¿”å›é»˜è®¤æ¨¡å‹')
        return [{ id: currentProvider.value.defaultModel, name: currentProvider.value.defaultModel }]
      }
      
      return models
      
    } catch (error) {
      console.warn('è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥:', error.message)
      // è¿”å›é»˜è®¤æ¨¡å‹ä½œä¸ºåå¤‡
      return [{ id: currentProvider.value.defaultModel, name: currentProvider.value.defaultModel }]
    }
  }

  /**
   * æ¸…ç©ºèŠå¤©å†å²
   */
  function clearChat() {
    chatMessages.value = []
  }

  /**
   * é‡ç½®ç»Ÿè®¡
   */
  function resetStats() {
    stats.value = {
      totalCalls: 0,
      successCalls: 0,
      failedCalls: 0,
      totalTokens: 0
    }
  }

  // ----- å¯¼å‡º -----

  return {
    // é…ç½®
    provider,
    apiKey,
    apiEndpoint,
    model,
    enabled,
    isConfigured,
    currentProvider,
    saveConfig,
    switchProvider,
    
    // çŠ¶æ€
    isGenerating,
    chatMessages,
    stats,
    
    // æ ¸å¿ƒ AI åŠŸèƒ½
    callAI,
    generateCommand,
    diagnoseError,
    analyzeLogs,
    recommendWorkflow,
    chat,
    executeIntelligentTask,
    
    // è¾…åŠ©åŠŸèƒ½
    fetchAvailableModels,
    clearChat,
    resetStats
  }
})
